# bayesian_GMM_MH Bayesian inference with a Gaussian mixture model (for 1D data)
#
#   bayesian_GMM_MH fits a mixture density consisting of a pre-specified number of
#   Gaussian components to the data vector x. It does so in a
#   Bayesian way: the output consists of samples from the posterior
#   distribution of the mixture density parameters. 
#   For each Gaussian component, bayesian_GMM_MH returns samples from the posterior 
#   distributions of three variables:
#
#       - the mean
#       - the standard deviation
#       - the proportion of data points in the sample 
#         generated by this component
#         (similar to the mixing coefficient or probability
#           in a standard mixture model)
#
#   The prior on the allocations of data points to components
#   is such that each component has at least 2 data points 
#   assigned to it in every iteration. This enables the improper Jeffreys prior
#   p(mu, sigma) proportional to sigma^(-1) to be used for the component means 
#   and variances. For more details see the accompanying paper.
#
#   GMM_MH uses a Metropolis-Hastings based sampler. 
#   GMM_CG implements the same model using a collapsed Gibbs sampler.
#   GMM_MH has the advantage that it will tend to have faster convergence,
#   especially for data with modes that are considerable distances apart.
#   However, it may require multiple proposal distributions to be tuned. The
#   advantage of GMM_CG is that it does not require any such tuning, as it
#   is based on Gibbs sampling.
#
#   Example function call:
#
#   results <-  bayesian_GMM_MH(x, K=2, min_mean=-5, max_mean=5, min_sd=0.01,
#                 start_mean=[-2 2], start_sd=[1 1],
#                 scale_mean=1, scale_sd=0.1,
#                 thin=10^2, burn_in=10^4, post_burn_in=10^5)
#
#   Arguments:
#
#   x             :   the data vector. This should be at least approximately
#                     centered (mean zero).
#
#     The other arguments are specified via name-value pairs, as in the example, 
#     and can be in any order. Anything designated OPTIONAL has a default
#     value, anything else is mandatory.
#
#   K             :   the number of components to use
#
#   start_mean    :   the starting value for the means
#
#   start_sd      :   the starting value for the standard deviations
#
#   min_mean      :   the minimum value the component means can take
#
#   max_mean      :   the maximum value the component means can take
#                     (during the Metropolis-Hastings random walk, 
#                     if any of the means leaves the interval [min_mean, max_mean],
#                     then it wraps around to the other end)
#
#   min_sd        :   the minimum value the component standard deviations 
#                     can take (during the Metropolis-Hastings random
#                     walk, if any proposal sd is smaller than this,
#                     it is reflected around the minimum value)
#
#   delta         :   the concentration parameter of the prior on
#                     the latent variables G
#                     (analogous to the concentration parameter 
#                      of a symmetric Dirichlet distribution) 
#                     OPTIONAL (default = 1)
#
#   scale_mean    :   the standard deviation of the normal proposal distribution
#                     for the component means
#
#   scale_sd      :   the standard deviation of the normal proposal distribution
#                     for the component standard deviations
#
#   thin          :   the interval at which to store samples from the 
#                     Markov chain
#
#   burn_in       :   the number of iterations to initially run the 
#                     Markov chain, without storing samples
#   
#   post_burn_in  :   the number of iterations to run after burn_in,
#                     storing samples at the interval given by thin
#   
#   progress_bar  :   whether to display a progress bar
#                     OPTIONAL (default = true)
#
#   Returns:
#
#
#   mean_samples  :   a S by K matrix of samples from the means of the components 
#
#   sd_samples    :   a S by K matrix of samples from the standard deviations of the
#                     components
#
#   prop_samples  :   a S by K matrix of samples from the proportions
#                     with which the components contribute to the data
#                     (n/N)
#
#   acceptances   :   the number of proposal moves accepted by the Metropolis-
#                     Hastings scheme after burn-in
#                     (useful for determining good proposal distribution scales)
#
#   Here, S is the number of samples, given by S = post_burn_in/thin
#
#   Starting point choice:
#
#   The starting means and standard deviations should be reasonably good fits
#   to the data.
#
#   Proposal distribution tuning: 
#
#   scale_mean and scale_sd should be adjusted based on the range of the data (i.e. what
#   proposal move length seems reasonable to explore the range of component
#   means and standard deviations).

bayesian_GMM_MH <- function(x, K, start_mean, min_mean, max_mean, start_sd, min_sd, scale_mean, scale_sd, delta=1, thin, burn_in, post_burn_in, progress_bar=TRUE){
  
  mean_curr <- start_mean
  sd_curr   <- start_sd
  
  N <- length(x)
  
  #store values of the log-gamma function  
  #note that we have to index into these with n + 1
  #to account for the fact the function of 0 has index 1  
  gammaln_delta <- lgamma((0:N) + delta)

  #make a matrix of N columns
  #where each column is a data point in x repeated K times  
  x_mat <- t(matrix(rep(x, K), N, K))
  
  sample_number <- post_burn_in/thin
  mean_samples  <- matrix(0, sample_number, K)
  sd_samples    <- matrix(0, sample_number, K)
  prop_samples  <- matrix(0, sample_number, K)
  acceptances   <- 0
  
  #variables for progress bar and sample storage
  pos     <- 1
  r       <- 1
  bar_pos <- 0
  bars    <- 0
  
  if(progress_bar){
    bar_length <- 50
    bar_number <- 100
    if((burn_in + post_burn_in) < (bar_length*bar_number)){
      bar_number <- 10
    }
    abovebar <- paste(rep('_', bar_length), collapse='')
    cat('Progress:\n')
    cat(abovebar)
    cat(sprintf(" %3d %%\n", 0))
    report_time     <- cumsum(regular_partition(burn_in + post_burn_in, bar_length*bar_number))
    len_report_time <- length(report_time)
  }
  
  n_prop <- matrix(K, 1)
  
  for (i in 1:(burn_in + post_burn_in) ){
    if(i==1){
      #in the first round, work from starting values of mean, sd and w
      #use these to get a proposal for z
      mean_prop   <- mean_curr
      sd_prop     <- sd_curr
      log_sd_prop <- log(sd_prop)
    }
    else{
      #get a proposal mean vector
      #by drawing from a normal proposal density
      #and doing a wrap-around so all components are in [min_mean, max_mean]
      mean_prop <- mean_curr + rnorm(K)*scale_mean
      while(TRUE){
        below_min <- mean_prop < min_mean
        above_max <- mean_prop > max_mean
        if(sum(below_min)==0 && sum(above_max)==0){
          break
        }
        else{
          mean_prop[below_min] <- max_mean - (min_mean - mean_prop[below_min])
          mean_prop[above_max] <- min_mean + (mean_prop[above_max] - max_mean)
        }
      }

      #get a proposal sd vector
      #by drawing from a normal proposal density
      #and reflecting sd values that are smaller than the minimum value
      #around the minimum value
      sd_prop <- sd_curr + rnorm(K)*scale_sd
      below_min <- sd_prop < min_sd
      sd_prop[below_min] <- 2*min_sd - sd_prop[below_min]
      log_sd_prop <- log(sd_prop)
    }

    
    #now need to get a proposal z
    #bad_n is true if there is a Gaussian component with fewer than
    #2 data points assigned to it
    bad_n <- 0
    
    while(1){
      #compute the proposal distribution for z
      #this is a matrix, the (i,j)-th entry is P(z(j) = i)
      #i.e. the j-th column is the vector of probabilities for the 
      #j-th entry of z
      #calling mapply(op, m, v) applies (op v) to rows of m

      diff_xm      <- matrix(mapply("-", x_mat, mean_prop), K, N)
      diff_scaled2 <- matrix(mapply("/", diff_xm, sd_prop), K, N)^2
      log_z_prop_P <- matrix(mapply("+", -0.5*diff_scaled2, -log_sd_prop), K, N)
      z_prop_P     <- exp(log_z_prop_P)
      z_prop_P     <- scale(z_prop_P, center = FALSE, scale = colSums(z_prop_P))
      
      #now generate a new proposal z
      u <- runif(N)
      z_prop  <-  colSums(matrix(mapply("<", apply(z_prop_P, 2, cumsum), u), K, N)) + 1
    
      #compute n(z) for the proposal
      #i.e. the number of data points assigned to each component
      n_prop <- hist(z_prop, breaks=(0:K + 0.5), plot=FALSE)$counts
      
      #register whether a component received fewer than 2 data points
      #(i.e. whether z has prior probability zero)
      bad_n <- sum(n_prop >= 2) < K
      
      #if this is the first iteration, loop until we have generated a z
      #in which each component receives at least 2 data points
      if(i>1 || !bad_n){
        break
      }
    }
    
    #if bad_n is true, the proposal for z has zero prior probability
    #hence it has zero posterior probability
    #and we automatically can reject this proposal
    #otherwise, we may possibly accept it, so continue
    
    if(!bad_n){
      #compute the log proposal density
      #only take the part depending on z because the other proposal densities
      #are symmetric and cancel
      log_g_prop <- sum(log(z_prop_P[cbind(z_prop, 1:N)]))
      
      #the target density: the joint posterior of means, variances and z 
      #this is the likelihood of x (given means, variances and z) times
      #the priors 
      log_P_prop <- -sum(log_sd_prop[z_prop]) - sum(log_sd_prop) - 0.5*sum( ( (x - mean_prop[z_prop])/sd_prop[z_prop] )^2 ) + sum(gammaln_delta[n_prop + 1])
      
      #if this is the first iteration, automatically accept
      if(i == 1){
        n_curr <- n_prop
        log_g_curr <- log_g_prop
        log_P_curr <- log_P_prop
      }
      else{
        #do a Metropolis-Hastings probabilistic acceptance
        A <- exp(log_g_curr - log_g_prop + log_P_prop - log_P_curr)
        if(A >= 1){
          accept = 1
        }
        else{
          accept <- runif(1) < A
        }
        if(accept){
          mean_curr  <- mean_prop
          sd_curr    <- sd_prop
          n_curr     <- n_prop
          log_g_curr <- log_g_prop
          log_P_curr <- log_P_prop
          
          if(i > burn_in){
            acceptances <- acceptances + 1
          }
        }
      }
    }
    #store a sample of the parameters,
    #at appropriate intervals
    if(i > burn_in && (i-burn_in)%%thin==0){
      mean_samples[pos,] <- mean_curr
      sd_samples[pos,]   <- sd_curr
      prop_samples[pos,] <- n_curr/N
      pos <- pos + 1
    }
    while(progress_bar && r<=len_report_time && i >= report_time[r]){
      r <- r + 1
      cat('-')
      bar_pos <- bar_pos + 1
      if(bar_pos == bar_length){
        bars <- bars + 1
        cat(sprintf(" %3d %%\n", floor(bars/bar_number*100+0.001)))
        bar_pos <- 0
      }
    }
  }  
  if(progress_bar){
    cat(sprintf("\n"))
  }
  list("mean_samples"=mean_samples, "sd_samples"=sd_samples, "prop_samples"=prop_samples, "acceptances"=acceptances)
}
