% The MIT License (MIT)
%
% Copyright (c) 2014 Colin J. Stoneking
%
% Permission is hereby granted, free of charge, to any person obtaining a copy
% of this software and associated documentation files (the "Software"), to deal
% in the Software without restriction, including without limitation the rights
% to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
% copies of the Software, and to permit persons to whom the Software is
% furnished to do so, subject to the following conditions:
%
% The above copyright notice and this permission notice shall be included in
% all copies or substantial portions of the Software.
%
% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
% AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
% OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
% THE SOFTWARE.

function [mean_samples, sd_samples, prop_samples, acceptances] = bayesian_GMM_MH(x, varargin)
% bayesian_GMM_MH Bayesian inference with a Gaussian mixture model (for 1D data)
%
%   bayesian_GMM_MH fits a mixture density consisting of a pre-specified number of
%   Gaussian components to the data vector x. It does so in a
%   Bayesian way: the output consists of samples from the posterior
%   distribution of the mixture density parameters. 
%   For each Gaussian component, bayesian_GMM_MH returns samples from the posterior 
%   distributions of three variables:
%
%       - the mean
%       - the standard deviation
%       - the proportion of data points in the sample 
%         generated by this component
%         (similar to the mixing coefficient or probability
%           in a standard mixture model)
%
%   The prior on the allocations of data points to components
%   is such that each component has at least 2 data points 
%   assigned to it in every iteration. This enables the improper Jeffreys prior
%   p(mu, sigma) proportional to sigma^(-1) to be used for the component means 
%   and variances. For more details see the accompanying paper.
%
%   GMM_MH uses a Metropolis-Hastings based sampler. 
%   GMM_CG implements the same model using a collapsed Gibbs sampler.
%   GMM_MH has the advantage that it will tend to have faster convergence,
%   especially for data with modes that are considerable distances apart.
%   However, it may require multiple proposal distributions to be tuned. The
%   advantage of GMM_CG is that it does not require any such tuning, as it
%   is based on Gibbs sampling.
%
%   Example function call:
%
%   [mean_samples, sd_samples, prop_samples, acceptances] =...
%       bayesian_GMM_MH(x, 'K', 2, 'min_mean', -5, 'max_mean', 5, 'min_sd', 0.01,...
%       'start_mean', [-2 2], 'start_sd', [1 1],...
%       'scale_mean', 1, 'scale_sd', 0.1,...
%       'thin', 10^2, 'burn_in', 10^4, 'post_burn_in', 10^5)
%
%   Arguments:
%
%   x             :   the data vector. This should be at least approximately
%                     centered (mean zero).
%
%     The other arguments are specified via name-value pairs, as in the example, 
%     and can be in any order. Anything designated OPTIONAL has a default
%     value, anything else is mandatory.
%
%   K             :   the number of components to use
%
%   start_mean    :   the starting value for the means
%
%   start_sd      :   the starting value for the standard deviations
%
%   min_mean      :   the minimum value the component means can take
%
%   max_mean      :   the maximum value the component means can take
%                     (during the Metropolis-Hastings random walk, 
%                     if any of the means leaves the interval [min_mean, max_mean],
%                     then it wraps around to the other end)
%
%   min_sd        :   the minimum value the component standard deviations 
%                     can take (during the Metropolis-Hastings random
%                     walk, if any proposal sd is smaller than this,
%                     it is reflected around the minimum value)
%
%   delta         :   the concentration parameter of the prior on
%                     the latent variables G
%                     (analogous to the concentration parameter 
%                      of a symmetric Dirichlet distribution) 
%                     OPTIONAL (default = 1)
%
%   scale_mean    :   the standard deviation of the normal proposal distribution
%                     for the component means
%
%   scale_sd      :   the standard deviation of the normal proposal distribution
%                     for the component standard deviations
%
%   thin          :   the interval at which to store samples from the 
%                     Markov chain
%
%   burn_in       :   the number of iterations to initially run the 
%                     Markov chain, without storing samples
%   
%   post_burn_in  :   the number of iterations to run after burn_in,
%                     storing samples at the interval given by thin
%   
%   progress_bar  :   whether to display a progress bar
%                     OPTIONAL (default = true)
%
%   Returns:
%
%
%   mean_samples  :   a S by K matrix of samples from the means of the components 
%
%   sd_samples    :   a S by K matrix of samples from the standard deviations of the
%                     components
%
%   prop_samples  :   a S by K matrix of samples from the proportions
%                     with which the components contribute to the data
%                     (n/N)
%
%   acceptances   :   the number of proposal moves accepted by the Metropolis-
%                     Hastings scheme after burn-in
%                     (useful for determining good proposal distribution scales)
%
%   Here, S is the number of samples, given by S = post_burn_in/thin
%
%   Starting point choice:
%
%   The starting means and standard deviations should be reasonably good fits
%   to the data.
%
%   Proposal distribution tuning: 
%
%   scale_mean and scale_sd should be adjusted based on the range of the data (i.e. what
%   proposal move length seems reasonable to explore the range of component
%   means and standard deviations).


%set default values of optional parameters here:
delta           = 1;
progress_bar    = 1;

v = 1;

while(v<=length(varargin))
    if(strcmpi(varargin{v}, 'min_mean'))
        min_mean = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'max_mean'))
        max_mean = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'min_sd'))
        min_sd   = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'K'))
        K = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'delta'))
        delta = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'scale_mean'))
        scale_mean = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'scale_sd'))
        scale_sd = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'start_mean'))
        start_mean = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'start_sd'))
        start_sd = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'thin'))
        thin = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'burn_in'))
        burn_in = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'post_burn_in'))
        post_burn_in = varargin{v+1};
        v = v + 1;
    elseif(strcmpi(varargin{v}, 'progress_bar'))
        progress_bar = 1;
    else
        err = MException('BayesGMM:arg',...
            ['unknown argument: ' varargin{v}]);
        throw(err); 
    end
    v = v + 1;
end

%the 'curr' variables are the current values of mean, sd etc.
%the 'prop' variables are the proposal values for these
mean_curr    = start_mean;
sd_curr      = start_sd;

if(isrow(x))
    x = x';
end
if(isrow(mean_curr))
    mean_curr = mean_curr';
end
if(isrow(sd_curr))
    sd_curr = sd_curr';
end

N = length(x);

%store values of the log-gamma function
%note that we always have to index into these with n + 1 
%to account for the fact that the function of 0 has index 1
gammaln_delta = gammaln((0:1:N) + delta);

%create a matrix of N columns
%where each column is a data point in x repeated K times
x_mat = repmat(x', K, 1);

sample_number   = post_burn_in/thin;
mean_samples    = zeros(sample_number, K);
sd_samples      = zeros(sample_number, K);
prop_samples    = zeros(sample_number, K);
acceptances     = 0;

%variables for progress bar and sample storage
pos     = 1;
r       = 1;
bar_pos = 0;
bars    = 0;

if(progress_bar)
    bar_length = 50;
    bar_number = 100;
    if((burn_in + post_burn_in)<(bar_length*bar_number))
        bar_number = 10;
    end
    abovebar = repmat('_', 1, bar_length);
    fprintf('Progress:\n');
    fprintf(abovebar);
    fprintf('%3d%%\n',0);
    report_time = cumsum(regular_partition(burn_in + post_burn_in, bar_length*bar_number));
    len_report_time = length(report_time);
end

n_prop = zeros(K, 1);

for i = 1:(burn_in + post_burn_in)
    if(i==1)
        %in the first round, work from starting values of mean and sd
        %use these to get a proposal for z
        mean_prop   = mean_curr;
        sd_prop     = sd_curr;
        log_sd_prop = log(sd_prop);
    else
        %get a proposal mean vector
        %by drawing from a normal proposal density
        %and doing a wrap-around so all components are in [min_mean, max_mean]
        mean_prop = mean_curr + randn(K, 1)*scale_mean;
        while(1)
            below_min = mean_prop < min_mean;
            above_max = mean_prop > max_mean;
            if(sum(below_min)==0 && sum(above_max)==0)
                break;
            else
                mean_prop(below_min) = max_mean - (min_mean - mean_prop(below_min));
                mean_prop(above_max) = min_mean + (mean_prop(above_max) - max_mean);
            end
        end
    
        %get a proposal sd vector
        %by drawing from a normal proposal density
        %and reflecting sd values that are smaller than the minimum value
        %around the minimum value
        sd_prop = sd_curr + randn(K,1)*scale_sd;
        below_min = sd_prop < min_sd;
        sd_prop(below_min) = 2*min_sd - sd_prop(below_min);
        log_sd_prop = log(sd_prop); 
    end
    
    %now need to get a proposal G  
    %bad_n is true if there is a Gaussian component with fewer than
    %2 data points assigned to it
    bad_n = 0;
    
    while(1)
        
        %compute the proposal distribution for G, called G_prop_P
        %this is a matrix, the (i,j)-th entry is P(G(j) = i)
        %i.e. the j-th column is the vector of probabilities for the 
        %j-th entry of G
        log_G_prop_P = bsxfun(@plus, -0.5*(bsxfun(@rdivide, bsxfun(@minus, x_mat, mean_prop), sd_prop)).^2, -log_sd_prop); 
        %subtract the maximum entry in each column from that column
        %to avoid numeric overflow
        log_G_prop_P = bsxfun(@minus, log_G_prop_P, max(log_G_prop_P));
        %exp and normalize to obtain probabilities
        G_prop_P     = exp(log_G_prop_P);
        G_prop_P     = bsxfun(@rdivide, G_prop_P, sum(G_prop_P));
    
        %now generate a new proposal G
        %by randomly sampling one index for each column of G_prop_P
        G_prop = sum(bsxfun(@lt, cumsum(G_prop_P), rand(1, N))) + 1; 
       
        %compute n(G) for the proposal
        %i.e. the number of data points assigned to each component
        n_prop = histc(G_prop, 1:K);
    
        %register whether a component received fewer than 2 data points
        %(i.e. whether G has prior probability zero)
        bad_n = sum(n_prop >= 2) < K;
        
        %if this is the first iteration, loop until we
        %have generated a G in which each component receives at least
        %2 data points
        if(i>1 || ~bad_n)
            break;
        end
    end
        
    %if bad_n is true, the proposal for G has zero prior probability
    %hence it has zero posterior probability
    %and we automatically can reject this proposal
    %otherwise, we may possibly accept it, so continue
    if(~bad_n)
        
        %compute the log proposal density
        %only take the part depending on G because the other proposal densities
        %are symmetric and cancel
        log_g_prop = sum(log(G_prop_P(sub2ind([K N], G_prop, 1:N))));
        
        %log_P_prop is the log target density, which is the joint posterior of means, sds and G 
        %this is the likelihood of x (given means, sds and G) times
        %the priors        
        log_P_prop  = - sum(log_sd_prop(G_prop)) - sum(log_sd_prop)...
                      - 0.5*sum( ((x - mean_prop(G_prop))./sd_prop(G_prop)).^2 )...
                      + sum(gammaln_delta(n_prop + 1));
        
        %if this is the first iteration, automatically accept
        if(i==1)
            n_curr      = n_prop;
            log_g_curr  = log_g_prop;
            log_P_curr  = log_P_prop;
        else
            %do a Metropolis-Hastings probabilistic acceptance        
            A = exp(log_g_curr - log_g_prop +  log_P_prop - log_P_curr);
            
            if(A >= 1)
                accept = 1; 
            else
                accept = rand < A;
            end
            if(accept)
                
                mean_curr   = mean_prop;
                sd_curr     = sd_prop;
                n_curr      = n_prop;
                log_g_curr  = log_g_prop;
                log_P_curr  = log_P_prop;                
                
                if(i > burn_in)
                    acceptances = acceptances + 1;
                end
            end
        end
    end
    
    %store a sample of the parameters,
    %at appropriate intervals
    if((i>burn_in) && (mod(i-burn_in, thin)==0))
        mean_samples(pos, :) = mean_curr';
        sd_samples(pos, :)   = sd_curr';
        prop_samples(pos, :) = n_curr'/N;  
        pos = pos + 1;
    end
    
    while(progress_bar && r<=len_report_time && i>=report_time(r))
        r = r + 1; 
        fprintf('-');
        bar_pos = bar_pos + 1;
        if(bar_pos == bar_length)
            bars = bars + 1;
            fprintf('%3d%%\n', floor(bars/bar_number*100+0.001));
            bar_pos = 0;
        end
    end
end
if(progress_bar)
    fprintf('\n');
end


end

