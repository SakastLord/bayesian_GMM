% The MIT License (MIT)
%
% Copyright (c) 2014 Colin J. Stoneking
%
% Permission is hereby granted, free of charge, to any person obtaining a copy
% of this software and associated documentation files (the "Software"), to deal
% in the Software without restriction, including without limitation the rights
% to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
% copies of the Software, and to permit persons to whom the Software is
% furnished to do so, subject to the following conditions:
%
% The above copyright notice and this permission notice shall be included in
% all copies or substantial portions of the Software.
%
% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
% AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
% OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
% THE SOFTWARE.

function [mean_samples, sd_samples, prop_samples] = bayesian_GMM_CG(x, varargin)
% bayesian_GMM_CG Bayesian inference with a Gaussian mixture model (for 1D data)
%
%   bayesian_GMM_CG fits a mixture density consisting of a pre-specified number of
%   Gaussian components to the data vector x. It does so in a
%   Bayesian way: the output consists of samples from the posterior
%   distribution of the mixture density parameters. 
%   For each Gaussian component, bayesian_GMM_CG returns samples from the posterior 
%   distributions of three variables:
%
%       - the mean
%       - the standard deviation
%       - the proportion of data points in the sample 
%         generated by this component
%         (similar to the mixing coefficient or probability
%           in a standard mixture model)
%
%   The prior on the allocations of data points to components
%   is such that each component has at least 2 data points 
%   assigned to it in every iteration. This enables the improper Jeffreys prior
%   p(mu, sigma) proportional to sigma^(-1) to be used for the component means 
%   and variances. For more details see the accompanying paper.
%
%   GMM_CG uses a collapsed Gibbs sampler.
%   GMM_MH implements the same model using a Metropolis-Hastings based sampler. 
%   GMM_MH has the advantage that it will tend to have faster convergence,
%   especially for data with modes that are considerable distances apart.
%   However, it may require multiple proposal distributions to be tuned. The
%   advantage of GMM_CG is that it does not require any such tuning, as it
%   is based on Gibbs sampling.
%
%   Example function call:
%
%   [mean_samples, sd_samples, prop_samples] =...
%       bayesian_GMM_CG(x, 'K', 2, 'thin', 10^2, 'burn_in', 10^4, 'post_burn_in', 10^5)
%
%   Arguments:
%
%   x             :   the data vector. This should be at least approximately
%                     centered (mean zero).
%
%     The other arguments are specified via name-value pairs, as in the example, 
%     and can be in any order. Anything designated OPTIONAL has a default
%     value, anything else is mandatory.
%
%   K             :   the number of components to use
%
%   delta         :   the concentration parameter of the prior on
%                     the latent variables G
%                     (analogous to the concentration parameter 
%                      of a symmetric Dirichlet distribution) 
%                     OPTIONAL (default = 1)
%
%   thin          :   the interval at which to store samples from the 
%                     Markov chain
%
%   burn_in       :   the number of iterations to initially run the 
%                     Markov chain, without storing samples
%   
%   post_burn_in  :   the number of iterations to run after burn_in,
%                     storing samples at the interval given by thin
%   
%   progress_bar  :   whether to display a progress bar
%                     OPTIONAL (default = true)
%
%   Returns:
%
%
%   mean_samples  :   a S by K matrix of samples from the means of the components 
%
%   sd_samples    :   a S by K matrix of samples from the standard deviations of the
%                     components
%
%   prop_samples  :   a S by K matrix of samples from the proportions
%                     with which the components contribute to the data
%                     (n/N)
%
%   Here, S is the number of samples, given by S = post_burn_in/thin
%
%   This implementation also has the capability to use a 
%   normal-inverse gamma prior. To do so, set improper to 0, and specify
%   values for the prior parameters alpha, beta and kappa (meanings as in
%   the paper). Note that in this case, prop_samples actually has samples
%   of the parameter p.


%set default values of optional parameters here:
delta = 1;
improper = 1;

v = 1;

while(v<=length(varargin))
    if(strcmp(varargin{v}, 'K'))
        K = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'delta'))
        delta = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'thin'))
        thin = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'burn_in'))
        burn_in = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'post_burn_in'))
        post_burn_in = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'progress_bar'))
        progress_bar = 1;
    elseif(strcmp(varargin{v}, 'improper'))
        improper = 1;
    elseif(strcmp(varargin{v}, 'alpha'))
        alpha = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'beta'))
        beta = varargin{v+1};
        v = v + 1;
    elseif(strcmp(varargin{v}, 'kappa'))
        kappa = varargin{v+1};
        v = v + 1;
    else
        err = MException('BayesGMM:arg',...
            ['unknown argument: ' varargin{v}]);
        throw(err); 
    end
    v = v + 1;
end

if(improper)
    min_size = 2;
else
    min_size = 0;
end

N = length(x);

sample_number = post_burn_in/thin;

mean_samples    = zeros(sample_number, K);
sd_samples      = zeros(sample_number, K);
prop_samples    = zeros(sample_number, K);
%zero_samples    = zeros(sample_number, 1);

%variables for progress bar and sample storage
pos     = 1;
r       = 1;
bar_pos = 0;
bars    = 0;

if(progress_bar)
    bar_length = 50;
    bar_number = 100;
    if((burn_in + post_burn_in)<(bar_length*bar_number))
        bar_number = 10;
    end
    abovebar = repmat('_', 1, bar_length);
    fprintf('Progress:\n');
    fprintf(abovebar);
    fprintf('%3d%%\n',0);
    report_time = cumsum(regular_partition(burn_in + post_burn_in, bar_length*bar_number));
end

x2 = x.^2;

%select initial cluster size at random
%by adding a multinomial variate
%to a vector of minimal cluster sizes
multnom = histc(rand(N - min_size*K, 1), [(0:1:(K-1))/K Inf]);
multnom = multnom(1:K);
initial_sizes = ones(K, 1)*min_size + multnom;

z = zeros(N,1);

%now allocate x values to latent variables at random
indices = randperm(N);
i = 1;
for j = 1:K
    z(indices(i:(i + initial_sizes(j) - 1))) = j;
    i = i + initial_sizes(j);
end

%now compute means and mean squares
%for the different groups
m  = zeros(1, K);
m2 = zeros(1, K);
n  = zeros(1, K);

logpost = zeros(1, K);

for j = 1:K
    zj      = (z == j);
    xj      = x(zj);
    n(j)    = sum(zj);
    m(j)    = mean(xj);
    m2(j)   = mean(xj.^2);
end

%the log-likelihood difference has a part that depends only on group size
%precompute it here
%this is the log-likelihood difference between a group with i members
%and a group with i-1 members
loglike_diff_npart = zeros(1, N);

if(~improper)
    for i = 1:N
       loglike_diff_npart(i) = -((i+1)/2 + alpha)*log(i + kappa) + gammaln(i/2 + alpha)...
                         -( -(i/2 + alpha)*log(i - 1 + kappa) + gammaln((i-1)/2 + alpha));
    end
else
    for i = 1:N
        loglike_diff_npart(i) = - i/2*log(i) + gammaln((i-1)/2)...
                           -( -(i-1)/2*log(i-1) + gammaln((i-2)/2) );
        %note that the first 2 entries in this will be infinite
    end
end

%also precompute log of prior ratio 
%(for the prior on z)
logprior = log((1:N) + delta - 1)';

for i = 1:(burn_in + post_burn_in)
    
    %visit each entry in z in turn, and sample a new value for it
    for j = 1:N
        %get the old group assignment, and group size
        old = z(j);
        nold = n(old);

        %check if we cannot change this assignment
        %because otherwise one group would be too small
        if(nold == min_size && min_size > 0)
            continue;
        end
        
        xj  =  x(j);
        x2j = x2(j);
        
        nnew = nold - 1;
        
        if(nnew > 0)        
            %take this x out of its current group,
            %and recompute group mean and mean square
            q = nold/nnew;
            m(old)  = q*m(old) - xj/nnew;
            m2(old) = q*m2(old) - x2j/nnew;
        else
           %this can only happen when we use a proper prior
           %and allow groups to have any size
           m(old)  = 0;
           m2(old) = 0;
        end
        
        n(old)  = nnew;
        
        %now compute the log-posterior probability
        %for each possible inclusion of the data point in a different group
        %we subtract off many constant terms
        %the "loglike" we compute below is just the log of the marginal
        %density ratio f(x, x_j)/f(x), with x containing all points in
        %group h
        for h = 1:K
            %compute group mean and mean square
            %for the h-th group with and without this data point
            nh   = n(h);
            mh   = m(h);
            m2h  = m2(h);
            nt   = nh + 1;
            mt   = nh/nt*m(h) +  xj/nt;
            m2t  = nh/nt*m2(h) + x2j/nt;
            
            if(~improper)
                st = nt/(nt + kappa);
                sh = nh/(nh + kappa);
                loglike  = - (nt/2 + alpha) * log(st*m2t - (st*mt)^2 + 2*beta/(nt + kappa))...
                         -(- (nh/2 + alpha) * log(sh*m2h - (sh*mh)^2 + 2*beta/(nh + kappa)))...
                         + loglike_diff_npart(nt);
            else
                loglike  = (1-nt)/2 * log(m2t-mt^2) - (1-nh)/2 * log(m2h-mh^2) + loglike_diff_npart(nt);
            end
            logpost(h) = loglike + logprior(nt);    
        end
        logpost = logpost - max(logpost);
        posterior = exp(logpost);
        
        %sample a new group
        %to allocate this x to
        group = find(rand < cumsum(posterior/sum(posterior)), 1, 'first');
        z(j) = group;
        
        %update the mean and mean squared for this group
        nold = n(group);
        nnew = nold + 1;
        q = nold/nnew;
        m(group)    = q*m(group) + xj/nnew;
        m2(group)   = q*m2(group) + x2j/nnew;
        n(group)    = nnew;
   end

   if((i>burn_in) && (mod(i-burn_in, thin)==0))
       %store a sample of the parameters
       %by sampling from normal-inverse gamma distribution
       if(~improper)
           %normal-inverse gamma prior
           q = n./(n + kappa);
           sigma = sqrt(1./gamrnd(alpha + 0.5*n, 1./( beta + 0.5*(n + kappa).*(q.*m2 - (q.*m).^2))));
           mu  = q.*m + randn(1, K).*sigma./sqrt(n + kappa);
           prop = randdirichlet(1, n + delta);
       else
           %Jeffreys prior
           sigma = sqrt( 1./gamrnd( 0.5*(n-1), 1./(0.5*n.*(m2-m.^2) ) ) );
           mu = m + randn(1, K).*sigma./sqrt(n);
           prop = n/N;
       end
       mean_samples(pos, :) = mu;
       sd_samples(pos, :)   = sigma;
       prop_samples(pos, :) = prop;
       %zero_samples(pos) = sum(n==0);
       pos = pos + 1;
   end
    
   while(progress_bar && r<=length(report_time) && i>=report_time(r))
       r = r + 1; 
       fprintf('-');
       bar_pos = bar_pos + 1;
       if(bar_pos == bar_length)
           bars = bars + 1;
           fprintf('%3d%%\n', floor(bars/bar_number*100+0.001));
           bar_pos = 0;
       end
   end
end
if(progress_bar)
    fprintf('\n');
end

end

